{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   words\n",
      "KARACHI          karachi\n",
      "Jan.                jan.\n",
      "Human-caus  human-caused\n",
      "climat           climate\n",
      "chang            changes\n"
     ]
    }
   ],
   "source": [
    "#Code to read a CSV file and produce the clusters using LDA\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Read in the data from the CSV into Python\n",
    "def unicode_csv_reader(utf8_data, dialect=csv.excel, **kwargs):\n",
    "    csv_reader = csv.reader(utf8_data, dialect=dialect, **kwargs)\n",
    "    for row in csv_reader:\n",
    "        yield [unicode(cell, 'utf-8') for cell in row]\n",
    "\n",
    "filename = 'C:\\Users\\Andrea\\Documents\\PhD\\Journal paper 2016\\data.csv'\n",
    "reader = unicode_csv_reader(open(filename))\n",
    "header = reader.next()\n",
    "\n",
    "news_articles = []\n",
    "\n",
    "for row in reader:\n",
    "    if row[3]==\"Jan\":\n",
    "        news_articles.append(row[0])\n",
    "#end: Read in the data from the CSV into Python\n",
    "\n",
    "\n",
    "#Text Parsing: use NLTK to create the stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#adding words to the stop word list manually\n",
    "stopwords.append('said')\n",
    "stopwords.append(\"'s\")\n",
    "stopwords.append('wa')\n",
    "stopwords.append('ha')\n",
    "stopwords.append('thi')\n",
    "stopwords.append('per')\n",
    "stopwords.append('his')\n",
    "stopwords.append('any')\n",
    "\n",
    "#Text Parsing: use NLTK to stem the text\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "#Define a function that will tokenize and stem the text using Porter\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [porter.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "#Define a function that will tokenize the text\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in news_articles:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'news_articles', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "\n",
    "\n",
    "#create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column. \n",
    "#The benefit of this is it provides an efficient way to look up a stem and return a full token. \n",
    "#The downside here is that stems to tokens are one to many: the stem 'run' could be associated with 'ran', 'runs', 'running', etc. \n",
    "#For my purposes this is fine--I'm perfectly happy returning the first token associated with the stem I need to look up.\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print vocab_frame.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165, 370)\n",
      "[u'organ', u'poor', u'life', u'thousand', u'oper', u'said new dengu', u'parti', u'suffer', u'polic', u'polio', u'sri lanka ani', u'lanka ani queri', u'program', u'proper', u'case wa detect', u'dr shakeel', u'report sindh provinc', u'senior', u'result', u'direct', u'feder', u'said total dengu', u'industri', u'weather', u'visit', u'karachi jan.', u'wa detect', u'home', u'follow', u'victim']\n"
     ]
    }
   ],
   "source": [
    "#Create the TF-IDF matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "#define vectorizer parameters; using n-grams here\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, max_features=200000,\n",
    "                                 min_df=0.10, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(news_articles) #fit the vectorizer to news articles\n",
    "print(tfidf_matrix.shape)\n",
    "indices = np.argsort(tfidf_vectorizer.idf_)[::-1]\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "top_n = 30\n",
    "top_features = [features[i] for i in indices[:top_n]]\n",
    "print top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 87 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    48\n",
       "4    46\n",
       "3    29\n",
       "1    24\n",
       "0    18\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#k means clustering, outputs number of documents per cluster\n",
    "from sklearn.cluster import KMeans\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "num_clusters = 5\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "%time km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "films = { 'cluster': clusters}\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['cluster'])\n",
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['government'], ['the'], ['punjab'], ['he'], ['year'], ['people'], ['changes'], ['increase'], ['countries'], ['it'], ['also'], ['province'], ['diseases'], ['like'], ['water'], ['would'], ['climate'], ['during'], ['any'], ['last']]\n",
      "()\n",
      "[['case'], ['report'], ['the'], ['fever'], ['year'], ['karachi'], ['sindh'], ['malaria'], ['mosquito'], ['people'], ['viral'], ['patient'], ['number'], ['i'], ['he'], ['one'], ['dr'], ['province'], ['any'], ['january']]\n",
      "()\n",
      "[['the'], ['health'], ['health'], ['diseases'], ['year'], ['also'], ['he'], ['pakistan'], ['public'], ['case'], ['countries'], ['government'], ['polio'], ['medical'], ['any'], ['dr'], ['patient'], ['prevention'], ['areas'], ['last']]\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "#Running LDA \n",
    "import string\n",
    "from gensim import corpora, models, similarities \n",
    "\n",
    "#tokenize -- necessary step; we don't use tf-idf for LDA\n",
    "tokenized_text = [tokenize_and_stem(text) for text in news_articles]\n",
    "\n",
    "#remove stop words\n",
    "texts = [[word for word in text if word not in stopwords] for text in tokenized_text]\n",
    "\n",
    "#create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "from gensim import models \n",
    "lda = models.LdaModel(corpus, num_topics=3, id2word=dictionary, update_every=5, chunksize=10000, passes=100)\n",
    "\n",
    "lda.show_topics()\n",
    "topics_matrix = lda.show_topics(formatted=False, num_words=20)\n",
    "topics_matrix = np.array(topics_matrix, dtype=object)\n",
    "topic_words = topics_matrix[:,1]\n",
    "\n",
    "for i in topic_words:\n",
    "    print([[str(vocab_frame.loc[word[0]].ix[0,0])] for word in i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    72\n",
       "2    64\n",
       "0    29\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docTopic = lda.get_document_topics(corpus,minimum_probability=0)\n",
    "listDocProb = list(docTopic)\n",
    "\n",
    "probMatrix = np.zeros(shape=(165,3))\n",
    "for i,x in enumerate(listDocProb):      #each document i\n",
    "    for t in x:     #each topic j\n",
    "        probMatrix[i, t[0]] = t[1] \n",
    "\n",
    "df = pd.DataFrame(probMatrix)\n",
    "\n",
    "top_n = 1\n",
    "topic_d = pd.DataFrame({n: df.T[col].nlargest(top_n).index.tolist() \n",
    "                  for n, col in enumerate(df.T)}).T\n",
    "topic_d.columns = ['count']\n",
    "topic_d['count'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#end goal is to output the clusters and then use tableau to create those clusters"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
